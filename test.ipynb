{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "arguments :\n",
    "first argument: location of test csv , format should be x1(premise) , x2(hypo) , target\n",
    "see data folder for example csv\n",
    "\n",
    "second argument: whether to use mode 0  or mode1(prediction each example in testset)\n",
    "in mode 0, it will only report test set accuracy\n",
    "in mode 1, it will give output for each example in test set\n",
    "\n",
    "\"\"\"\n",
    "test_set_file=sys.argv[1]\n",
    "mode=sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import util.yelp as yelp\n",
    "import numpy as np\n",
    "from util.text_util import normalize\n",
    "\n",
    "from hnatt import HNATT\n",
    "\n",
    "#YELP_DATA_PATH = 'data/yelp-dataset/yelp.csv'\n",
    "SAVED_MODEL_DIR = 'saved_models'\n",
    "SAVED_MODEL_FILENAME = 'model.h5'\n",
    "EMBEDDINGS_PATH = 'saved_models/glove.6B.100d.txt'\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('reading ' + test_set_file + ' .. ')\n",
    "    df_test=pd.read_csv(test_set_file)\n",
    "except:\n",
    "    print('couldnot read csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "class labels:\n",
    "0 : entails\n",
    "2 : contradiction\n",
    "1 : neutral\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"x1\"] = df_test[\"x1\"] + \" \" + df_test[\"x2\"]\n",
    "df_test['target']=df_test['target'].apply(data_classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_text = 'x1'\n",
    "col_target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dim=5):\n",
    "\tresults = np.zeros((len(labels), dim))\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tresults[i][label - 1] = 1\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(text):\n",
    "    ntext = normalize(text)\n",
    "    preds = h.predict([ntext])[0]\n",
    "    prediction = np.argmax(preds).astype(float)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "try:\n",
    "    print('loading pretrained model ..')\n",
    "    h = HNATT()\n",
    "    h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "except:\n",
    "    print('unable to load pretrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode==0:\n",
    "    df_test['text_tokens']=df_test['x1'].progress_apply(lambda x: normalize(x))\n",
    "\n",
    "#df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "    test_x = np.empty((0,))\n",
    "    test_y = np.empty((0,))\n",
    "\n",
    "    test_x=df_test['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "    test_y = to_one_hot(y_test, dim=3)\n",
    "    print(h.model.metrics_names)\n",
    "    # test on test set\n",
    "    loss_and_metrics = h.test(test_x, test_y, batch_size=64)\n",
    "    print(loss_and_metrics)\n",
    "    #print(test_x[0])\n",
    "    #print(test_y[0])\n",
    "else:\n",
    "    print('running mode 1 , printing ouput for each test example')\n",
    "    for row in df_test.itertuples():\n",
    "        print(row.x1)\n",
    "        print(predict_single(row.x1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-10243b851a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnatt_keras",
   "language": "python",
   "name": "hnatt_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
