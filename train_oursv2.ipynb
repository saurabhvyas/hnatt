{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import util.yelp as yelp\n",
    "import numpy as np\n",
    "from util.text_util import normalize\n",
    "\n",
    "from hnatt import HNATT\n",
    "\n",
    "YELP_DATA_PATH = 'data/yelp-dataset/yelp.csv'\n",
    "SAVED_MODEL_DIR = 'saved_models'\n",
    "SAVED_MODEL_FILENAME = 'model.h5'\n",
    "EMBEDDINGS_PATH = 'saved_models/glove.6B.100d.txt'\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('concatenated_train_pandas.csv')\n",
    "#df_valid=pd.read_csv('concatenated_dev_pandas.csv')\n",
    "df_test=pd.read_csv('concatenated_test_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man in a jersey waiting on a crowded subway platform.\n",
      "the man just came back from a game\n"
     ]
    }
   ],
   "source": [
    "print(df[\"x1\"][0])\n",
    "print(df[\"x2\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x1\"] = df[\"x1\"] + \" \" + df[\"x2\"]\n",
    "#df_valid[\"x1\"] = df_valid[\"x1\"] + \" \" + df_valid[\"x2\"]\n",
    "df_test[\"x1\"] = df_test[\"x1\"] + \" \" + df_test[\"x2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man in a jersey waiting on a crowded subway platform. the man just came back from a game\n"
     ]
    }
   ],
   "source": [
    "print(df[\"x1\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels into class indices\n",
    "data_classes = [\"entails\", \"neutral\", \"contradiction\"]\n",
    "df['target']=df['target'].apply(data_classes.index)\n",
    "#df_valid['target']=df_valid['target'].apply(data_classes.index)\n",
    "df_test['target']=df_test['target'].apply(data_classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "#df_valid=df_valid.dropna()\n",
    "df_test=df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[:int(df.shape[0]*0.001)]\n",
    "#df_valid=df_valid[:int(df_valid.shape[0]*0.001)]\n",
    "#df_test=df_test[:int(df_test.shape[0]*0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_text = 'x1'\n",
    "col_target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[col_target]\n",
    "y_test = df_test[col_target]\n",
    "#y_val = val[col_target].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x=df[\"x1\"]\n",
    "#train_y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nh = HNATT()\\t\\nh.train(train_x, train_y, \\nbatch_size=16,\\nepochs=16,\\nembeddings_path=None)\\n\\n#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\\n\\n\\n\\n# print attention activation maps across sentences and words per sentence\\nactivation_maps = h.activation_maps(\\n'they have some pretty interesting things here. i will definitely go back again.')\\nprint(activation_maps)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "h = HNATT()\t\n",
    "h.train(train_x, train_y, \n",
    "batch_size=16,\n",
    "epochs=16,\n",
    "embeddings_path=None)\n",
    "\n",
    "#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_x, train_y), (test_x, test_y) = yelp.load_data(path='yelp.csv', size=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dim=5):\n",
    "\tresults = np.zeros((len(labels), dim))\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tresults[i][label - 1] = 1\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572458/572458 [01:41<00:00, 5621.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'man in jersey waiting on crowded subway platform', u'man just came back from game']\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x = np.empty((0,))\n",
    "train_y = np.empty((0,))\n",
    "\n",
    "training_len=df['x1'].shape[0]\n",
    "\n",
    "#train_set = df[0:training_len].copy()\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x=df['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "train_y = to_one_hot(y_train, dim=3)\n",
    "print(train_x[0])\n",
    "print(train_y[0])\n",
    "\n",
    "#test_y = to_one_hot(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11950/11950 [00:02<00:00, 4228.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'these girls are having great time looking for seashells', u'girls are happy']\n",
      "[0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess test_x , as above block\n",
    "\n",
    "df_test['text_tokens']=df_test['x1'].progress_apply(lambda x: normalize(x))\n",
    "\n",
    "#df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "test_x = np.empty((0,))\n",
    "test_y = np.empty((0,))\n",
    "\n",
    "test_x=df_test['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "test_y = to_one_hot(y_test, dim=3)\n",
    "print(test_x[0])\n",
    "print(test_y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 100)\n",
      "(None, 50, 100)\n",
      "(None, 20, 100)\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "h = HNATT()\n",
    "h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "word_embeddings (Embedding)  (None, 50, 100)           4467700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_w (Dense)    (None, 50, 100)           10100     \n",
      "_________________________________________________________________\n",
      "word_attention (Attention)   (None, 100)               100       \n",
      "=================================================================\n",
      "Total params: 4,523,200\n",
      "Trainable params: 4,523,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 50, 100)\n",
      "(None, 20, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 100)           4523200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 20, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 4,579,003\n",
      "Trainable params: 4,579,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "11950/11950 [==============================] - 54s 5ms/step\n",
      "[1.098733713736594, 0.296317991636787]\n"
     ]
    }
   ],
   "source": [
    "# test on test set\n",
    "loss_and_metrics = h.test(test_x, test_y, batch_size=64)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = HNATT()\t\n",
    "h.train(train_x, train_y, \n",
    "batch_size=64,\n",
    "epochs=16,\n",
    "embeddings_path=None,saved_model_dir=SAVED_MODEL_DIR,\n",
    "saved_model_filename=SAVED_MODEL_FILENAME)\n",
    "\n",
    "#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "#activation_maps = h.activation_maps(\n",
    "#'they have some pretty interesting things here. i will definitely go back again.')\n",
    "#print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(u'they', 0.008062019428867473), (u'have', 0.015740954290724567), (u'some', 0.005001129248379999), (u'pretty', 0.39570732830754896), (u'interesting', 0.41721613859206297), (u'things', 0.0911523002322377), (u'here', 0.06712012990017838)], 0.13714904), ([(u'i', 0.03554345484665767), (u'will', 0.044393781401083576), (u'definitely', 0.5791513074525129), (u'go', 0.021449978624936066), (u'back', 0.012777409247274829), (u'again', 0.30668406842753504)], 0.86285096)]\n"
     ]
    }
   ],
   "source": [
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnatt_keras",
   "language": "python",
   "name": "hnatt_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
