{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import util.yelp as yelp\n",
    "import numpy as np\n",
    "from util.text_util import normalize\n",
    "\n",
    "from hnatt import HNATT\n",
    "\n",
    "#YELP_DATA_PATH = 'data/yelp-dataset/yelp.csv'\n",
    "SAVED_MODEL_DIR = 'saved_models'\n",
    "SAVED_MODEL_FILENAME = 'model.h5'\n",
    "EMBEDDINGS_PATH = 'saved_models/glove.6B.100d.txt'\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/preprocessing/preprocessing_scripts/output/concatenated_train_pandas.csv')\n",
    "df_valid=pd.read_csv('data/preprocessing/preprocessing_scripts/output/concatenated_dev_pandas.csv')\n",
    "df_test=pd.read_csv('data/concatenated_test_pandas.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back on the river, perched above a 150-m- (490-ft-) deep ravine, is the fairytale castle of Beynac-et-Cazenac.\n",
      "Cinderella's castle was based partly upon Beynac-et-Cazenac.\n"
     ]
    }
   ],
   "source": [
    "#print(df[\"x1\"][0])\n",
    "#print(df[\"x2\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x1\"] = df[\"x1\"] + \" \" + df[\"x2\"]\n",
    "df_valid[\"x1\"] = df_valid[\"x1\"] + \" \" + df_valid[\"x2\"]\n",
    "df_test[\"x1\"] = df_test[\"x1\"] + \" \" + df_test[\"x2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog climbing up and over and red wooden ramp. The dog is dead.\n"
     ]
    }
   ],
   "source": [
    "#print(df[\"x1\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man in a black jacket standing beside a green mailbox thoroughly soiled by a group of pigeons that are nearby. A man is outdoors\n"
     ]
    }
   ],
   "source": [
    "#print(df_test['x1'][6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        contradiction\n",
      "1              entails\n",
      "2              entails\n",
      "3              entails\n",
      "4              entails\n",
      "5              neutral\n",
      "6              entails\n",
      "7              entails\n",
      "8              entails\n",
      "9              entails\n",
      "10       contradiction\n",
      "11       contradiction\n",
      "12       contradiction\n",
      "13       contradiction\n",
      "14             entails\n",
      "15       contradiction\n",
      "16             neutral\n",
      "17             neutral\n",
      "18             entails\n",
      "19             neutral\n",
      "20       contradiction\n",
      "21             entails\n",
      "22             entails\n",
      "23       contradiction\n",
      "24             neutral\n",
      "25             neutral\n",
      "26             neutral\n",
      "27             entails\n",
      "28       contradiction\n",
      "29             entails\n",
      "             ...      \n",
      "20948          neutral\n",
      "20949          entails\n",
      "20950          entails\n",
      "20951          neutral\n",
      "20952    contradiction\n",
      "20953          entails\n",
      "20954    contradiction\n",
      "20955          entails\n",
      "20956    contradiction\n",
      "20957    contradiction\n",
      "20958          neutral\n",
      "20959    contradiction\n",
      "20960    contradiction\n",
      "20961    contradiction\n",
      "20962    contradiction\n",
      "20963          neutral\n",
      "20964    contradiction\n",
      "20965    contradiction\n",
      "20966    contradiction\n",
      "20967    contradiction\n",
      "20968    contradiction\n",
      "20969          neutral\n",
      "20970          entails\n",
      "20971          neutral\n",
      "20972          entails\n",
      "20973          entails\n",
      "20974          entails\n",
      "20975          neutral\n",
      "20976    contradiction\n",
      "20977          entails\n",
      "Name: target, Length: 20978, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df_valid['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in df_test.itertuples():\n",
    " #   print(row.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels into class indices\n",
    "data_classes = [\"entails\", \"neutral\", \"contradiction\"]\n",
    "df['target']=df['target'].apply(data_classes.index)\n",
    "df_valid['target']=df_valid['target'].apply(data_classes.index)\n",
    "df_test['target']=df_test['target'].apply(data_classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     2\n",
      "2     2\n",
      "3     1\n",
      "4     1\n",
      "5     0\n",
      "6     1\n",
      "7     1\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    1\n",
      "12    2\n",
      "13    0\n",
      "14    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(df_test['target'][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "df_valid=df_valid.dropna()\n",
    "df_test=df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[:int(df.shape[0]*0.001)]\n",
    "#df_valid=df_valid[:int(df_valid.shape[0]*0.001)]\n",
    "#df_test=df_test[:int(df_test.shape[0]*0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_text = 'x1'\n",
    "col_target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[col_target]\n",
    "y_test = df_test[col_target]\n",
    "y_val = df_valid[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#print(y_val[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x=df[\"x1\"]\n",
    "#train_y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nh = HNATT()\\t\\nh.train(train_x, train_y, \\nbatch_size=16,\\nepochs=16,\\nembeddings_path=None)\\n\\n#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\\n\\n\\n\\n# print attention activation maps across sentences and words per sentence\\nactivation_maps = h.activation_maps(\\n'they have some pretty interesting things here. i will definitely go back again.')\\nprint(activation_maps)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "h = HNATT()\t\n",
    "h.train(train_x, train_y, \n",
    "batch_size=16,\n",
    "epochs=16,\n",
    "embeddings_path=None)\n",
    "\n",
    "#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_x, train_y), (test_x, test_y) = yelp.load_data(path='yelp.csv', size=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dim=5):\n",
    "\tresults = np.zeros((len(labels), dim))\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tresults[i][label - 1] = 1\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965120/965120 [05:06<00:00, 3148.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'back on river perched above 150-m- 490-ft- deep ravine is fairytale castle of beynac et cazenac', u\"cinderella 's castle was based partly upon beynac et cazenac\"]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('loading training set ...')\n",
    "df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x = np.empty((0,))\n",
    "train_y = np.empty((0,))\n",
    "\n",
    "training_len=df['x1'].shape[0]\n",
    "\n",
    "#train_set = df[0:training_len].copy()\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x=df['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "train_y = to_one_hot(y_train, dim=3)\n",
    "print(train_x[0])\n",
    "print(train_y[0])\n",
    "\n",
    "#test_y = to_one_hot(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11950/11950 [00:03<00:00, 2997.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'these girls are having great time looking for seashells', u'girls are happy']\n",
      "[0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess test_x , as above block\n",
    "print('loading test set ...')\n",
    "df_test['text_tokens']=df_test['x1'].progress_apply(lambda x: normalize(x))\n",
    "\n",
    "#df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "test_x = np.empty((0,))\n",
    "test_y = np.empty((0,))\n",
    "\n",
    "test_x=df_test['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "test_y = to_one_hot(y_test, dim=3)\n",
    "print(test_x[0])\n",
    "print(test_y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20978/20978 [00:07<00:00, 2781.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'woman prepares to strike volleyball', u'woman is shooting basket through hoop']\n",
      "[0. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#preprocess val_x , as above block\n",
    "print('loading validation set ...')\n",
    "\n",
    "df_valid['text_tokens']=df_valid['x1'].progress_apply(lambda x: normalize(x))\n",
    "\n",
    "#df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "valid_x = np.empty((0,))\n",
    "valid_y = np.empty((0,))\n",
    "\n",
    "valid_x=df_valid['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "valid_y = to_one_hot(y_val, dim=3)\n",
    "print(valid_x[0])\n",
    "print(valid_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 100)\n",
      "(None, 50, 100)\n",
      "(None, 20, 100)\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "print('loading pretrained model / restoring model ...')\n",
    "h = HNATT()\n",
    "h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11950/11950 [==============================] - 29s 2ms/step\n",
      "[0.76285937477854, 0.7003347279935701]\n"
     ]
    }
   ],
   "source": [
    "# test on test set\n",
    "#loss_and_metrics = h.test(test_x, test_y, batch_size=64)\n",
    "#print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#h.model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valid_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 965120 samples, validate on 20978 samples\n",
      "Epoch 1/16\n",
      " 30144/965120 [..............................] - ETA: 1:56:28 - loss: 0.7096 - acc: 0.6871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-126dbdf8021f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membeddings_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVED_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m saved_model_filename='mnli.h5')\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/hnatt.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_x, train_y, val_x, val_y, batch_size, epochs, embedding_dim, embeddings_path, saved_model_dir, saved_model_filename, restore)\u001b[0m\n\u001b[1;32m    253\u001b[0m                                            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_val_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m \t\t\t\t\t   shuffle=True)\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_encode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#h = HNATT()\t\n",
    "h.train(train_x, train_y,valid_x,valid_y,\n",
    "batch_size=64,\n",
    "epochs=16,\n",
    "embeddings_path=None,saved_model_dir=SAVED_MODEL_DIR,\n",
    "saved_model_filename='mnli.h5')\n",
    "\n",
    "#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "#activation_maps = h.activation_maps(\n",
    "#'they have some pretty interesting things here. i will definitely go back again.')\n",
    "#print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 100)           4523200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 20, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 4,579,003\n",
      "Trainable params: 4,579,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#h.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "text='he was not well, he stayed at home only. he didnt go to office '\n",
    "\n",
    "ntext = normalize(text)\n",
    "preds = h.predict([ntext])[0]\n",
    "prediction = np.argmax(preds).astype(float)\n",
    "print(prediction)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#he is dancing with joy because its his birthday. he is very happy\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
