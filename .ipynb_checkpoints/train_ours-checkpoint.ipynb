{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.yelp as yelp\n",
    "import numpy as np\n",
    "from util.text_util import normalize\n",
    "\n",
    "from hnatt import HNATT\n",
    "\n",
    "YELP_DATA_PATH = 'data/yelp-dataset/yelp.csv'\n",
    "SAVED_MODEL_DIR = 'saved_models'\n",
    "SAVED_MODEL_FILENAME = 'model.h5'\n",
    "EMBEDDINGS_PATH = 'saved_models/glove.6B.100d.txt'\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('concatenated_train_pandas.csv')\n",
    "#df_valid=pd.read_csv('concatenated_dev_pandas.csv')\n",
    "df_test=pd.read_csv('concatenated_test_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man in a jersey waiting on a crowded subway platform.\n",
      "the man just came back from a game\n"
     ]
    }
   ],
   "source": [
    "print(df[\"x1\"][0])\n",
    "print(df[\"x2\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x1\"] = df[\"x1\"] + \" \" + df[\"x2\"]\n",
    "#df_valid[\"x1\"] = df_valid[\"x1\"] + \" \" + df_valid[\"x2\"]\n",
    "df_test[\"x1\"] = df_test[\"x1\"] + \" \" + df_test[\"x2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man in a jersey waiting on a crowded subway platform. the man just came back from a game\n"
     ]
    }
   ],
   "source": [
    "print(df[\"x1\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels into class indices\n",
    "data_classes = [\"entails\", \"neutral\", \"contradiction\"]\n",
    "df['target']=df['target'].apply(data_classes.index)\n",
    "#df_valid['target']=df_valid['target'].apply(data_classes.index)\n",
    "df_test['target']=df_test['target'].apply(data_classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "#df_valid=df_valid.dropna()\n",
    "df_test=df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:int(df.shape[0]*0.001)]\n",
    "#df_valid=df_valid[:int(df_valid.shape[0]*0.001)]\n",
    "df_test=df_test[:int(df_test.shape[0]*0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_text = 'x1'\n",
    "col_target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[col_target]\n",
    "y_test = df_test[col_target]\n",
    "#y_val = val[col_target].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x=df[\"x1\"]\n",
    "#train_y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(572,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "word_embeddings (Embedding)  (None, 1, 100)            4600      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 100)            45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_w (Dense)    (None, 1, 100)            10100     \n",
      "_________________________________________________________________\n",
      "word_attention (Attention)   (None, 100)               100       \n",
      "=================================================================\n",
      "Total params: 60,100\n",
      "Trainable params: 60,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 1, 100)\n",
      "(None, 20, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 1)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 100)           60100     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 20, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 572)               57772     \n",
      "=================================================================\n",
      "Total params: 173,372\n",
      "Trainable params: 173,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (572,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-173f433ad3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m embeddings_path=None)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/hnatt.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_x, train_y, batch_size, epochs, embedding_dim, embeddings_path, saved_model_dir, saved_model_filename)\u001b[0m\n\u001b[1;32m    230\u001b[0m                                            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                                            \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m \t\t\t\t\t   shuffle=True)\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_encode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (572,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "h = HNATT()\t\n",
    "h.train(train_x, train_y, \n",
    "batch_size=16,\n",
    "epochs=16,\n",
    "embeddings_path=None)\n",
    "\n",
    "#h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/10000 [00:00<00:51, 192.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Yelp reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 690.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my wife took me here on my birthday for breakfast and it was excellent', u'weather was perfect which made sitting outside overlooking their grounds absolute pleasure', u'our waitress was excellent and our food arrived quickly on semi busy saturday morning', u'it looked like place fills up pretty quickly so earlier you get here better', u'do yourself favor and get their bloody mary', u\"it was phenomenal and simply best i 've ever had\", u\"i 'm pretty sure they only use ingredients from their garden and blend them fresh when you order it\", u'it was amazing', u'while everything on menu looks excellent i had white truffle scrambled eggs vegetable skillet and it was tasty and delicious', u'it came with 2 pieces of their griddled bread with was amazing and it absolutely made meal complete', u\"it was best toast i 've ever had\", u\"anyway i ca n't wait to go back\"]\n",
      "[0. 0. 0. 0. 1.]\n",
      "finished loading Yelp reviews\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = yelp.load_data(path='yelp.csv', size=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dim=5):\n",
    "\tresults = np.zeros((len(labels), dim))\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tresults[i][label - 1] = 1\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:00<00:00, 1815.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'man in jersey waiting on crowded subway platform', u'man just came back from game']\n",
      "[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['text_tokens'] = df['x1'].progress_apply(lambda x: normalize(x))\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x = np.empty((0,))\n",
    "train_y = np.empty((0,))\n",
    "\n",
    "training_len=df['x1'].shape[0]\n",
    "\n",
    "#train_set = df[0:training_len].copy()\n",
    "#train_set['len'] = train_set['text_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "train_x=df['text_tokens']\n",
    "#train_y=train_set['']\n",
    "\n",
    "\n",
    "\n",
    "train_y = to_one_hot(y_train, dim=3)\n",
    "print(train_x[0])\n",
    "print(train_y[0])\n",
    "\n",
    "#test_y = to_one_hot(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "word_embeddings (Embedding)  (None, 50, 100)           203400    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 50, 100)           45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_w (Dense)    (None, 50, 100)           10100     \n",
      "_________________________________________________________________\n",
      "word_attention (Attention)   (None, 100)               100       \n",
      "=================================================================\n",
      "Total params: 258,900\n",
      "Trainable params: 258,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 50, 100)\n",
      "(None, 2, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 2, 100)            258900    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 2, 100)            45300     \n",
      "_________________________________________________________________\n",
      "dense_transform_s (Dense)    (None, 2, 100)            10100     \n",
      "_________________________________________________________________\n",
      "sentence_attention (Attentio (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 314,703\n",
      "Trainable params: 314,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 514 samples, validate on 58 samples\n",
      "Epoch 1/16\n",
      "514/514 [==============================] - 5s 10ms/step - loss: 1.1001 - acc: 0.3016 - val_loss: 1.1066 - val_acc: 0.2586\n",
      "Epoch 2/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 1.0855 - acc: 0.3949 - val_loss: 1.1363 - val_acc: 0.2586\n",
      "Epoch 3/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 1.0378 - acc: 0.4280 - val_loss: 1.3536 - val_acc: 0.2931\n",
      "Epoch 4/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.8765 - acc: 0.5856 - val_loss: 1.6779 - val_acc: 0.3276\n",
      "Epoch 5/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.5981 - acc: 0.7335 - val_loss: 2.1621 - val_acc: 0.2931\n",
      "Epoch 6/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.4175 - acc: 0.8541 - val_loss: 3.3332 - val_acc: 0.2586\n",
      "Epoch 7/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.2534 - acc: 0.9144 - val_loss: 4.4850 - val_acc: 0.2759\n",
      "Epoch 8/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.1456 - acc: 0.9514 - val_loss: 5.1054 - val_acc: 0.2931\n",
      "Epoch 9/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0796 - acc: 0.9689 - val_loss: 5.9657 - val_acc: 0.2586\n",
      "Epoch 10/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0407 - acc: 0.9786 - val_loss: 6.3977 - val_acc: 0.2586\n",
      "Epoch 11/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0152 - acc: 0.9981 - val_loss: 7.0276 - val_acc: 0.2586\n",
      "Epoch 12/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 7.3633 - val_acc: 0.2759\n",
      "Epoch 13/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 7.4094 - val_acc: 0.2759\n",
      "Epoch 14/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 7.4360 - val_acc: 0.2759\n",
      "Epoch 15/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 7.4545 - val_acc: 0.2759\n",
      "Epoch 16/16\n",
      "514/514 [==============================] - 3s 5ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 7.4789 - val_acc: 0.2759\n",
      "(None, 50, 100)\n",
      "(None, 50, 100)\n",
      "(None, 2, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No such layer: time_distributed_1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-269b2351cef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m saved_model_filename=SAVED_MODEL_FILENAME)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVED_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVED_MODEL_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/hnatt.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, saved_model_dir, saved_model_filename)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Attention'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_attention_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_distributed_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \t\t\ttokenizer_path = os.path.join(\n\u001b[1;32m    133\u001b[0m \t\t\t\tsaved_model_dir, self._get_tokenizer_filename(saved_model_filename))\n",
      "\u001b[0;32m/home/saurabh/entailment/hnatt_keras/.venv/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   1889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: time_distributed_1"
     ]
    }
   ],
   "source": [
    "h = HNATT()\t\n",
    "h.train(train_x, train_y, \n",
    "batch_size=16,\n",
    "epochs=16,\n",
    "embeddings_path=None,saved_model_dir=SAVED_MODEL_DIR,\n",
    "saved_model_filename=SAVED_MODEL_FILENAME)\n",
    "\n",
    "h.load_weights(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# print attention activation maps across sentences and words per sentence\n",
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(u'they', 0.0008925280362393766), (u'have', 0.032588102350719884), (u'some', 0.0009490147160073135), ('UNK', 0.19523888546154394), ('UNK', 0.25572600087489594), ('UNK', 0.2847231947625904), ('UNK', 0.2298822737980031)], 0.52864015), ([('UNK', 0.36579270114917994), (u'will', 0.10649317242699129), ('UNK', 0.4154420874938223), (u'go', 0.039542247666702335), (u'back', 0.010942730119873347), (u'again', 0.06178706114343084)], 0.4713598)]\n"
     ]
    }
   ],
   "source": [
    "activation_maps = h.activation_maps(\n",
    "'they have some pretty interesting things here. i will definitely go back again.')\n",
    "print(activation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'man in jersey waiting on crowded subway platform',\n",
       " u'man just came back from game']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnatt_keras",
   "language": "python",
   "name": "hnatt_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
